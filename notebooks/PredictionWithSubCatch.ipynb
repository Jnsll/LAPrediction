{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with Sub-Catchment Areas For Each Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import prediction_of_H_indicator_with_subCatchmentData as prediction\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FOLDER = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>SubCatch</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>LC</th>\n",
       "      <th>SAR</th>\n",
       "      <th>Area</th>\n",
       "      <th>CV</th>\n",
       "      <th>HV</th>\n",
       "      <th>HError</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.035656</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6770</th>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>8.875044</td>\n",
       "      <td>224.490265</td>\n",
       "      <td>933.385135</td>\n",
       "      <td>1.006683</td>\n",
       "      <td>4387500</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6771</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>6.585227</td>\n",
       "      <td>178.075058</td>\n",
       "      <td>700.135624</td>\n",
       "      <td>1.003876</td>\n",
       "      <td>2930625</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6772</th>\n",
       "      <td>40</td>\n",
       "      <td>17</td>\n",
       "      <td>5.182087</td>\n",
       "      <td>173.802765</td>\n",
       "      <td>685.592278</td>\n",
       "      <td>1.002990</td>\n",
       "      <td>3661875</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6773</th>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>4.617903</td>\n",
       "      <td>196.079285</td>\n",
       "      <td>597.577939</td>\n",
       "      <td>1.002033</td>\n",
       "      <td>5023125</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6774</th>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>2.886285</td>\n",
       "      <td>115.072578</td>\n",
       "      <td>1073.758334</td>\n",
       "      <td>1.000678</td>\n",
       "      <td>697500</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6775 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Site  SubCatch     Slope   Elevation           LC       SAR     Area  \\\n",
       "0        1         1  1.203898    8.936689   319.676414  1.000254  1648125   \n",
       "1        1         1  1.203898    8.936689   319.676414  1.000254  1648125   \n",
       "2        1         1  1.203898    8.936689   319.676414  1.000254  1648125   \n",
       "3        1         1  1.203898    8.936689   319.676414  1.000254  1648125   \n",
       "4        1         1  1.203898    8.936689   319.676414  1.000254  1648125   \n",
       "...    ...       ...       ...         ...          ...       ...      ...   \n",
       "6770    40        15  8.875044  224.490265   933.385135  1.006683  4387500   \n",
       "6771    40        16  6.585227  178.075058   700.135624  1.003876  2930625   \n",
       "6772    40        17  5.182087  173.802765   685.592278  1.002990  3661875   \n",
       "6773    40        18  4.617903  196.079285   597.577939  1.002033  5023125   \n",
       "6774    40        19  2.886285  115.072578  1073.758334  1.000678   697500   \n",
       "\n",
       "       CV   HV    HError  Rate  \n",
       "0     167  211  0.000000   1.0  \n",
       "1     167  211       NaN   2.0  \n",
       "2     167  211       NaN   7.0  \n",
       "3     167  211       NaN  15.0  \n",
       "4     167  211  0.035656  21.0  \n",
       "...   ...  ...       ...   ...  \n",
       "6770    0   12       NaN   NaN  \n",
       "6771    0   20       NaN   NaN  \n",
       "6772    0   34       NaN   NaN  \n",
       "6773    0   68       NaN   NaN  \n",
       "6774    0   14       NaN   NaN  \n",
       "\n",
       "[6775 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = prediction.import_input_data_clean()\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>SubCatch</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>LC</th>\n",
       "      <th>SAR</th>\n",
       "      <th>Area</th>\n",
       "      <th>CV</th>\n",
       "      <th>HV</th>\n",
       "      <th>HError</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.035656</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.083538</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.116415</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>8.758309</td>\n",
       "      <td>162.546722</td>\n",
       "      <td>690.708947</td>\n",
       "      <td>1.006050</td>\n",
       "      <td>2925000</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>7.371158</td>\n",
       "      <td>150.872406</td>\n",
       "      <td>470.062314</td>\n",
       "      <td>1.004910</td>\n",
       "      <td>8943750</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>4.720726</td>\n",
       "      <td>195.869247</td>\n",
       "      <td>672.920859</td>\n",
       "      <td>1.001445</td>\n",
       "      <td>1299375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>6.162337</td>\n",
       "      <td>121.253716</td>\n",
       "      <td>589.868107</td>\n",
       "      <td>1.003319</td>\n",
       "      <td>9225000</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>5.605823</td>\n",
       "      <td>217.943283</td>\n",
       "      <td>640.819977</td>\n",
       "      <td>1.001974</td>\n",
       "      <td>5394375</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Site  SubCatch     Slope   Elevation          LC       SAR     Area  \\\n",
       "0        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "4        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "8        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "9        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "11       1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "...    ...       ...       ...         ...         ...       ...      ...   \n",
       "6603    39         4  8.758309  162.546722  690.708947  1.006050  2925000   \n",
       "6633    39         5  7.371158  150.872406  470.062314  1.004910  8943750   \n",
       "6664    39         7  4.720726  195.869247  672.920859  1.001445  1299375   \n",
       "6694    39         8  6.162337  121.253716  589.868107  1.003319  9225000   \n",
       "6724    39         9  5.605823  217.943283  640.819977  1.001974  5394375   \n",
       "\n",
       "       CV   HV    HError   Rate  \n",
       "0     167  211  0.000000    1.0  \n",
       "4     167  211  0.035656   21.0  \n",
       "8     167  211  0.083538   60.0  \n",
       "9     167  211  0.093477   75.0  \n",
       "11    167  211  0.116415  100.0  \n",
       "...   ...  ...       ...    ...  \n",
       "6603    0   27  0.000000    1.0  \n",
       "6633    0  127  0.000000    1.0  \n",
       "6664    0    5  0.000000    1.0  \n",
       "6694    0   61  0.000000    1.0  \n",
       "6724    0   26  0.000000    1.0  \n",
       "\n",
       "[3435 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_cleaned = prediction.clean_data(input_data)\n",
    "input_data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the new site to make predictions for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaeed99e2be74779aa1f9398603705e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Site:', index=6, options=(('Breville-Sur-Mer', 1), ('Agon-Coutainville', 2), ('Saint-Ger…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef0e3c4884846ac9061c96dacce338c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Valid', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ecbd20991e40f8ba20bc246503b77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CAUTION : if running from jupyterlba => jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "## IF this cell is not running => SKIP THIS CELL and GO TO NEXT CELL :D\n",
    "\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "drop = widgets.Dropdown(\n",
    "    options=[('Breville-Sur-Mer', 1), ('Agon-Coutainville', 2), ('Saint-Germain-Sur-Ay', 3), ('Jullouville', 4), ('Etréham', 5), ('Genêts', 6), ('Saint-Pair-sur-Mer', 7), ('Arromanches-les-Bains', 8), ('Port-en-Bessin', 9), ('Courtils', 10), ('Lessay', 11), ('Doville', 12), ('Graye-sur-Mer', 13), ('Saint-Potan', 14), ('Saint-Vaast-la-Hougue', 15), ('La_Pernelle', 16), ('Lestre', 17), ('Banville', 18), ('Isigny-sur-Mer', 19), ('Saint-Malo', 20), ('Pierreville', 21), ('La_Feuillie', 22), ('Tourlaville', 23), ('Octeville', 24), ('Saint-Briac-sur-Mer', 25), ('Granville', 26), ('Blainville-sur-Mer', 27), ('Hauteville-sur-Mer', 28), ('Sainte-Marie-du-Mont', 29), ('Vierville-sur-Mer', 30), ('Cherrueix', 31), ('Chef-du-Pont', 32), ('Saint-Lô', 33), ('Couvains', 34), ('Rocheville', 35), ('Lison', 36), ('Marigny', 37), ('Carville', 38), ('Percy', 39), ('Saint-Armand', 40)],\n",
    "    value=7,\n",
    "    description='Site:',\n",
    ")\n",
    "button = widgets.Button(description=\"Valid\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(drop, button, output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global site_number\n",
    "    site_number = drop.value\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually change the site to predict the H values for ;p\n",
    "site_number = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets for training the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, variable_to_predict = prediction.split_dataset_into_features_and_variable_to_predict(input_data_cleaned)\n",
    "#print(features, variable_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data_for_a_subcatch(X, y, test_site, test_subcatch):\n",
    "    y_test = y.loc[(y.Site == test_site) & (y.SubCatch == test_subcatch)]\n",
    "        ## We do not want to take the site number into account for the prediction\n",
    "    del y_test[\"Site\"]\n",
    "    del y_test[\"SubCatch\"]\n",
    "\n",
    "        # Removing the data for the site we want to predict  =>>> /!\\ DONE : remove only the test subcatch of the test site !\n",
    "    #print(y[y.Site == test_site].index)\n",
    "    #print(y.loc[(y.Site == test_site) & (y.SubCatch == test_subcatch)].index)\n",
    "    y_train = y.drop(y.loc[(y.Site == test_site)].index)\n",
    "        ## We do not want to take the site number into account for the prediction\n",
    "    del y_train[\"Site\"]\n",
    "    del y_train[\"SubCatch\"]\n",
    "\n",
    "        # Splitting the x (features) into training and testing data\n",
    "    X_test = X.loc[(X.Site == test_site) & (X.SubCatch == test_subcatch)]\n",
    "        ## We do not want to take the site number into account for the prediction\n",
    "    del X_test[\"Site\"]\n",
    "    del X_test[\"SubCatch\"]\n",
    "\n",
    "        # Removing the data for the site we want to predict\n",
    "    X_train = X.drop(X.loc[(X.Site == test_site)].index)\n",
    "        ## We do not want to take the site number into account for the prediction\n",
    "    del X_train[\"Site\"]\n",
    "    del X_train[\"SubCatch\"]\n",
    "\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Slope  Elevation          LC      SAR     Area  CV  HV    Rate\n",
      "629  4.36158  43.982922  817.560286  1.00163  4471875  35  54     1.0\n",
      "630  4.36158  43.982922  817.560286  1.00163  4471875  35  54     2.0\n",
      "631  4.36158  43.982922  817.560286  1.00163  4471875  35  54     7.0\n",
      "632  4.36158  43.982922  817.560286  1.00163  4471875  35  54    15.0\n",
      "633  4.36158  43.982922  817.560286  1.00163  4471875  35  54    21.0\n",
      "634  4.36158  43.982922  817.560286  1.00163  4471875  35  54    30.0\n",
      "635  4.36158  43.982922  817.560286  1.00163  4471875  35  54    45.0\n",
      "636  4.36158  43.982922  817.560286  1.00163  4471875  35  54    50.0\n",
      "637  4.36158  43.982922  817.560286  1.00163  4471875  35  54    60.0\n",
      "638  4.36158  43.982922  817.560286  1.00163  4471875  35  54    75.0\n",
      "639  4.36158  43.982922  817.560286  1.00163  4471875  35  54    90.0\n",
      "640  4.36158  43.982922  817.560286  1.00163  4471875  35  54   100.0\n",
      "641  4.36158  43.982922  817.560286  1.00163  4471875  35  54   125.0\n",
      "642  4.36158  43.982922  817.560286  1.00163  4471875  35  54   150.0\n",
      "643  4.36158  43.982922  817.560286  1.00163  4471875  35  54   182.0\n",
      "644  4.36158  43.982922  817.560286  1.00163  4471875  35  54   200.0\n",
      "645  4.36158  43.982922  817.560286  1.00163  4471875  35  54   250.0\n",
      "646  4.36158  43.982922  817.560286  1.00163  4471875  35  54   300.0\n",
      "647  4.36158  43.982922  817.560286  1.00163  4471875  35  54   330.0\n",
      "648  4.36158  43.982922  817.560286  1.00163  4471875  35  54   365.0\n",
      "649  4.36158  43.982922  817.560286  1.00163  4471875  35  54   550.0\n",
      "650  4.36158  43.982922  817.560286  1.00163  4471875  35  54   640.0\n",
      "651  4.36158  43.982922  817.560286  1.00163  4471875  35  54   730.0\n",
      "652  4.36158  43.982922  817.560286  1.00163  4471875  35  54  1000.0\n",
      "653  4.36158  43.982922  817.560286  1.00163  4471875  35  54  1500.0\n",
      "654  4.36158  43.982922  817.560286  1.00163  4471875  35  54  2000.0\n",
      "655  4.36158  43.982922  817.560286  1.00163  4471875  35  54  2250.0\n",
      "656  4.36158  43.982922  817.560286  1.00163  4471875  35  54  3000.0\n",
      "657  4.36158  43.982922  817.560286  1.00163  4471875  35  54  3182.0\n",
      "658  4.36158  43.982922  817.560286  1.00163  4471875  35  54  3652.0        HError\n",
      "629  0.000000\n",
      "630  0.002527\n",
      "631  0.009512\n",
      "632  0.016053\n",
      "633  0.018748\n",
      "634  0.023852\n",
      "635  0.031748\n",
      "636  0.032878\n",
      "637  0.038381\n",
      "638  0.043167\n",
      "639  0.053638\n",
      "640  0.056546\n",
      "641  0.069232\n",
      "642  0.078636\n",
      "643  0.103605\n",
      "644  0.094220\n",
      "645  0.110429\n",
      "646  0.116814\n",
      "647  0.116449\n",
      "648  0.116676\n",
      "649  0.123356\n",
      "650  0.122262\n",
      "651  0.127822\n",
      "652  0.126052\n",
      "653  0.129826\n",
      "654  0.132835\n",
      "655  0.130451\n",
      "656  0.135461\n",
      "657  0.132959\n",
      "658  0.130211\n"
     ]
    }
   ],
   "source": [
    "subcatch_number = 4\n",
    "features_train, features_test, variable_train, variable_test = get_train_and_test_data_for_a_subcatch(features, variable_to_predict, site_number, subcatch_number)\n",
    "print(features_test, variable_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_model(X_train, y_train):\n",
    "    forest = RandomForestRegressor(\n",
    "        n_estimators=1000, criterion=\"mse\", random_state=1, n_jobs=-1, oob_score = True, bootstrap = True\n",
    "    )\n",
    "    forest.fit(X_train, y_train.values.ravel())\n",
    "    return forest\n",
    "\n",
    "prediction_model = train_random_forest_model(features_train, variable_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slope</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>LC</th>\n",
       "      <th>SAR</th>\n",
       "      <th>Area</th>\n",
       "      <th>CV</th>\n",
       "      <th>HV</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>8.758309</td>\n",
       "      <td>162.546722</td>\n",
       "      <td>690.708947</td>\n",
       "      <td>1.006050</td>\n",
       "      <td>2925000</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>7.371158</td>\n",
       "      <td>150.872406</td>\n",
       "      <td>470.062314</td>\n",
       "      <td>1.004910</td>\n",
       "      <td>8943750</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>4.720726</td>\n",
       "      <td>195.869247</td>\n",
       "      <td>672.920859</td>\n",
       "      <td>1.001445</td>\n",
       "      <td>1299375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>6.162337</td>\n",
       "      <td>121.253716</td>\n",
       "      <td>589.868107</td>\n",
       "      <td>1.003319</td>\n",
       "      <td>9225000</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>5.605823</td>\n",
       "      <td>217.943283</td>\n",
       "      <td>640.819977</td>\n",
       "      <td>1.001974</td>\n",
       "      <td>5394375</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3015 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Slope   Elevation          LC       SAR     Area   CV   HV   Rate\n",
       "0     1.203898    8.936689  319.676414  1.000254  1648125  167  211    1.0\n",
       "4     1.203898    8.936689  319.676414  1.000254  1648125  167  211   21.0\n",
       "8     1.203898    8.936689  319.676414  1.000254  1648125  167  211   60.0\n",
       "9     1.203898    8.936689  319.676414  1.000254  1648125  167  211   75.0\n",
       "11    1.203898    8.936689  319.676414  1.000254  1648125  167  211  100.0\n",
       "...        ...         ...         ...       ...      ...  ...  ...    ...\n",
       "6603  8.758309  162.546722  690.708947  1.006050  2925000    0   27    1.0\n",
       "6633  7.371158  150.872406  470.062314  1.004910  8943750    0  127    1.0\n",
       "6664  4.720726  195.869247  672.920859  1.001445  1299375    0    5    1.0\n",
       "6694  6.162337  121.253716  589.868107  1.003319  9225000    0   61    1.0\n",
       "6724  5.605823  217.943283  640.819977  1.001974  5394375    0   26    1.0\n",
       "\n",
       "[3015 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_model.feature_importances_    /!\\ USE permutation importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_trained_model(forest, X_train, X_test):\n",
    "    # Predicting results\n",
    "    y_train_pred = forest.predict(X_train)\n",
    "    y_test_pred = forest.predict(X_test)\n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "variable_train_pred, variable_test_pred = predict_with_trained_model(prediction_model, features_train, features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality metrics for H values (MSE & NSE/R² scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 0.014288007981789507} {4: -5.363992450388677}\n"
     ]
    }
   ],
   "source": [
    "subCatchment_numbers = prediction.get_subcatchment_numbers_for_a_subcatch(site_number, variable_to_predict, subcatch_number)\n",
    "liste_variable_test_HError = prediction.get_list_variable_test_Hind_Values(variable_test)\n",
    "liste_variable_test_pred_HError = variable_test_pred\n",
    "#print(subCatchment_numbers, liste_variable_test_HError, liste_variable_test_pred_HError)\n",
    "mse_test, r2_test = prediction.get_standard_quality_metrics(subCatchment_numbers, liste_variable_test_HError, liste_variable_test_pred_HError)\n",
    "print(mse_test, r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pmax real and Pmax predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>SubCatch</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>LC</th>\n",
       "      <th>SAR</th>\n",
       "      <th>Area</th>\n",
       "      <th>CV</th>\n",
       "      <th>HV</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>8.758309</td>\n",
       "      <td>162.546722</td>\n",
       "      <td>690.708947</td>\n",
       "      <td>1.006050</td>\n",
       "      <td>2925000</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>7.371158</td>\n",
       "      <td>150.872406</td>\n",
       "      <td>470.062314</td>\n",
       "      <td>1.004910</td>\n",
       "      <td>8943750</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>4.720726</td>\n",
       "      <td>195.869247</td>\n",
       "      <td>672.920859</td>\n",
       "      <td>1.001445</td>\n",
       "      <td>1299375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>6.162337</td>\n",
       "      <td>121.253716</td>\n",
       "      <td>589.868107</td>\n",
       "      <td>1.003319</td>\n",
       "      <td>9225000</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>5.605823</td>\n",
       "      <td>217.943283</td>\n",
       "      <td>640.819977</td>\n",
       "      <td>1.001974</td>\n",
       "      <td>5394375</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3437 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Site  SubCatch     Slope   Elevation          LC       SAR     Area  \\\n",
       "0        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "4        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "8        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "9        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "11       1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "...    ...       ...       ...         ...         ...       ...      ...   \n",
       "6603    39         4  8.758309  162.546722  690.708947  1.006050  2925000   \n",
       "6633    39         5  7.371158  150.872406  470.062314  1.004910  8943750   \n",
       "6664    39         7  4.720726  195.869247  672.920859  1.001445  1299375   \n",
       "6694    39         8  6.162337  121.253716  589.868107  1.003319  9225000   \n",
       "6724    39         9  5.605823  217.943283  640.819977  1.001974  5394375   \n",
       "\n",
       "       CV   HV   Rate  \n",
       "0     167  211    1.0  \n",
       "4     167  211   21.0  \n",
       "8     167  211   60.0  \n",
       "9     167  211   75.0  \n",
       "11    167  211  100.0  \n",
       "...   ...  ...    ...  \n",
       "6603    0   27    1.0  \n",
       "6633    0  127    1.0  \n",
       "6664    0    5    1.0  \n",
       "6694    0   61    1.0  \n",
       "6724    0   26    1.0  \n",
       "\n",
       "[3437 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 7.0, 15.0, 21.0, 30.0, 45.0, 50.0, 60.0, 75.0, 90.0, 100.0, 125.0, 150.0, 182.0, 200.0, 250.0, 300.0, 330.0, 365.0, 550.0, 640.0, 730.0, 1000.0, 1500.0, 2000.0, 2250.0, 3000.0, 3182.0, 3652.0] 30\n",
      "[0.0, 0.002526966035067, 0.009512015447923, 0.016052916554261, 0.018748325858716, 0.023852255699491, 0.031747779827179, 0.032877687915139, 0.038381191284593, 0.043166721775508, 0.053638078472765, 0.056545617622844, 0.069232078569536, 0.078635560739383, 0.103604689122397, 0.094220461256899, 0.110428507953097, 0.116813838960434, 0.116448998394691, 0.116676267527529, 0.123356306885072, 0.122262117667871, 0.127822366652954, 0.126051820731634, 0.129826277701743, 0.132835415015362, 0.130450582704871, 0.135461041100975, 0.132959066467299, 0.130211005584775] 30\n",
      "[0.         0.00344569 0.01582923 0.02294714 0.02540554 0.03222838\n",
      " 0.0514403  0.054902   0.06078319 0.07061313 0.09426985 0.09591724\n",
      " 0.12172878 0.15472735 0.19328758 0.18871854 0.20724732 0.21559709\n",
      " 0.21340844 0.21690003 0.27147154 0.26739517 0.28301808 0.28159019\n",
      " 0.31314103 0.34087162 0.33092584 0.35372609 0.35892901 0.35815029] 30\n",
      "15\n",
      "13\n",
      "Real value of p:  150.0\n",
      "Predicted value of p:  100.0\n"
     ]
    }
   ],
   "source": [
    "rates = prediction.get_rates_for_a_subcatch(site_number, features, subcatch_number)\n",
    "liste_variable_test_HError = prediction.get_list_variable_test_Hind_Values(variable_test)\n",
    "liste_variable_test_pred_HError = variable_test_pred\n",
    "subcatch = 4\n",
    "p_test = 0\n",
    "p_pred = 0\n",
    "H_limit = 0.1\n",
    "pmaxTest_found = False\n",
    "pmaxPred_found = False\n",
    "\n",
    "print(rates, len(rates))\n",
    "print(liste_variable_test_HError, len(liste_variable_test_HError))\n",
    "print(liste_variable_test_pred_HError, len(liste_variable_test_pred_HError))\n",
    "\n",
    "for index_sub in range(len(liste_variable_test_HError)):\n",
    "\n",
    "    if pmaxTest_found is False and liste_variable_test_HError[index_sub] > H_limit:\n",
    "        p_test = rates[index_sub -1]\n",
    "        pmaxTest_found = True\n",
    "    elif pmaxTest_found is False and index_sub == len(liste_variable_test_HError)-1:\n",
    "        if rates[-1] == float(3652):\n",
    "            p_test = 3652\n",
    "    elif pmaxTest_found:\n",
    "        print(index_sub)\n",
    "        break\n",
    "            \n",
    "for index_sub in range(len(liste_variable_test_pred_HError)):    \n",
    "    if pmaxPred_found is False and liste_variable_test_pred_HError[index_sub] > H_limit:\n",
    "        p_pred = rates[index_sub -1]\n",
    "        pmaxPred_found = True\n",
    "    elif pmaxPred_found is False and index_sub == len(liste_variable_test_pred_HError)-1:\n",
    "        if rates[-1] == float(3652):\n",
    "            p_pred = rates[-1]\n",
    "    elif pmaxPred_found:\n",
    "        print(index_sub)\n",
    "        break\n",
    "\n",
    "print(\"Real value of p: \", p_test)\n",
    "print(\"Predicted value of p: \", p_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 150.0} {4: 100.0}\n"
     ]
    }
   ],
   "source": [
    "rates = prediction.get_rates_for_a_subcatch(site_number, features, subcatch_number)\n",
    "pmax_test, pmax_pred = prediction.get_real_and_pred_pmax(subCatchment_numbers, rates, liste_variable_test_HError, variable_test_pred)\n",
    "print(pmax_test, pmax_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created here:  output/Approx0/Chronicle0/SiteTest5/Prediction_HErrorValues_SubCatch4_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HV_clean.csv\n",
      "File created here: output/Approx0/Chronicle0/SiteTest5/Prediction_PMax_SubCatch4_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HV_clean.csv\n"
     ]
    }
   ],
   "source": [
    "prediction.save_Hind_results_into_file_for_a_subcatch(site_number, subcatch_number, 1, subCatchment_numbers, rates, liste_variable_test_HError, variable_test_pred, approx=0, chronicle=0, permeability=27.32)\n",
    "prediction.save_Pmax_results_into_file_for_a_subcatch(site_number, subcatch_number, 1, pmax_test, pmax_pred, mse_test, r2_test, approx=0, chronicle=0, permeability=27.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import prediction_of_H_indicator_with_subCatchmentData as prediction\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def get_feature_importance_for_a_site_and_a_subcatch_no_CV(prediction_model, features_test, site_number, subcatch_number, nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "    sort_indexes_imp = np.argsort(prediction_model.feature_importances_)\n",
    "    #print(sort_indexes_imp[::-1])\n",
    "    \n",
    "    MYDIR = (\n",
    "        \"output\"\n",
    "        + \"/\"\n",
    "        + \"Approx\"\n",
    "        + str(approx)\n",
    "        + \"/Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"/SiteTest\"\n",
    "        + str(site_number)\n",
    "    )\n",
    "    with open(\n",
    "        MYDIR\n",
    "        + \"/\"\n",
    "        + \"Feature_Importance\"\n",
    "        + \"_Site\"\n",
    "        + str(site_number)\n",
    "        + \"_SubCatch\"\n",
    "        + str(subcatch_number)\n",
    "        +\"_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_HV\" #_CV_HV\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_clean.csv\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        writer = csv.writer(f, delimiter=\";\")\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"Approx\",\n",
    "                \"Chronicle\",\n",
    "                \"Test Site\",\n",
    "                \"SubCatchment\",\n",
    "                \"Feature\",\n",
    "                \"ImportanceValue\",\n",
    "                \"ImportanceRank\",\n",
    "            ]\n",
    "        )\n",
    "        rank=0\n",
    "        for i in sort_indexes_imp[::-1]:\n",
    "            rank+=1\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    approx,\n",
    "                    chronicle,\n",
    "                    site_number,\n",
    "                    subcatch_number,\n",
    "                    features_test.columns[i],\n",
    "                    prediction_model.feature_importances_[i],\n",
    "                    rank,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def prediction_worflow_for_a_site_and_subcatch(site_number, subcatch_number):\n",
    "    print(\"----Site\", site_number, \"Sub Catch\", subcatch_number, \"----\")\n",
    "    # Import input dataset & clean it\n",
    "    input_data = prediction.import_input_data_clean()\n",
    "    input_data_cleaned = prediction.clean_data(input_data)\n",
    "    # Split dataset\n",
    "    features, variable_to_predict = prediction.split_dataset_into_features_and_variable_to_predict(input_data_cleaned)\n",
    "    features_train, features_test, variable_train, variable_test = prediction.get_train_and_test_data_for_a_subcatch_no_CV(features, variable_to_predict, site_number, subcatch_number)\n",
    "    # Prediction\n",
    "    prediction_model = prediction.train_random_forest_model(features_train, variable_train)\n",
    "    variable_train_pred, variable_test_pred = prediction.predict_with_trained_model(prediction_model, features_train, features_test)\n",
    "    # Permutation Importance\n",
    "    #print(\"feature importance\", prediction_model.feature_importances_)\n",
    "    get_feature_importance_for_a_site_and_a_subcatch_no_CV(prediction_model, features_test, site_number, subcatch_number)\n",
    "\n",
    "    \n",
    "    #get_permut_importance(prediction_model, features_test, variable_test, site_number, subcatch_number)\n",
    "    \n",
    "    subCatchment_numbers = prediction.get_subcatchment_numbers_for_a_subcatch(site_number, variable_to_predict, subcatch_number)\n",
    "    liste_variable_test_HError = prediction.get_list_variable_test_Hind_Values(variable_test)\n",
    "    liste_variable_test_pred_HError = variable_test_pred\n",
    "    mse_test, r2_test = prediction.get_standard_quality_metrics(subCatchment_numbers, liste_variable_test_HError, liste_variable_test_pred_HError)\n",
    "    print(mse_test, r2_test)\n",
    "    rates = prediction.get_rates_for_a_subcatch(site_number, features, subcatch_number)\n",
    "    pmax_test, pmax_pred = prediction.get_real_and_pred_pmax(subCatchment_numbers, rates, liste_variable_test_HError, variable_test_pred)\n",
    "    print(pmax_test, pmax_pred)\n",
    "    prediction.save_Hind_results_into_file_for_a_subcatch_no_CV(site_number, subcatch_number, 1, subCatchment_numbers, rates, liste_variable_test_HError, variable_test_pred, approx=0, chronicle=0, permeability=27.32)\n",
    "    prediction.save_Pmax_results_into_file_for_a_subcatch_no_CV(site_number, subcatch_number, 1, pmax_test, pmax_pred, mse_test, r2_test, approx=0, chronicle=0, permeability=27.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_number in range(1,41):\n",
    "    for sub in range(1, 40):\n",
    "        try:\n",
    "            prediction_worflow_for_a_site_and_subcatch(site_number, sub)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_imp_for_a_site_and_all_subcatchs(site_number, nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "\n",
    "    imp_features_df_all = pd.DataFrame(columns=[\"Approx\",\n",
    "                \"Chronicle\",\n",
    "                \"Test Site\",\n",
    "                \"SubCatchment\",\n",
    "                \"Feature\",\n",
    "                \"ImportanceValue\",\n",
    "                \"ImportanceRank\"])\n",
    "    for subcatch_number in range(1, 40):\n",
    "        MYDIR = (\n",
    "            \"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "        )\n",
    "        file = (MYDIR\n",
    "        + \"/\"\n",
    "        + \"Feature_Importance\"\n",
    "        + \"_Site\"\n",
    "        + str(site_number)\n",
    "        + \"_SubCatch\"\n",
    "        + str(subcatch_number)\n",
    "        +\"_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_HV\" #_CV_HV\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_clean.csv\")\n",
    "        try:\n",
    "            imp_sub = pd.read_csv(file, sep=\";\")\n",
    "            imp_features_df_all = pd.concat([imp_features_df_all, imp_sub])\n",
    "        except:\n",
    "            continue\n",
    "    #print(imp_features_df_all)\n",
    "    \n",
    "    out_file =(\"Feature_Importance\" + \"_Site\"\n",
    "        + str(site_number)\n",
    "        + \"_All_SubCatchs\"\n",
    "        +\"_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_HV\" #_CV_HV\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_clean.csv\")\n",
    "    imp_features_df_all.to_csv(MYDIR + \"/\" + out_file, sep=\";\", index=False)\n",
    "    return imp_features_df_all\n",
    "\n",
    "#get_features_imp_for_a_site_and_all_subcatchs(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Approx</th>\n",
       "      <th>Chronicle</th>\n",
       "      <th>Test Site</th>\n",
       "      <th>SubCatchment</th>\n",
       "      <th>Feature</th>\n",
       "      <th>ImportanceValue</th>\n",
       "      <th>ImportanceRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Elevation</td>\n",
       "      <td>0.301368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Rate</td>\n",
       "      <td>0.257340</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>LC</td>\n",
       "      <td>0.149976</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SAR</td>\n",
       "      <td>0.089709</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Area</td>\n",
       "      <td>0.081249</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>SAR</td>\n",
       "      <td>0.083881</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>Area</td>\n",
       "      <td>0.070660</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>Slope</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>HV</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>CV</td>\n",
       "      <td>0.018755</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Approx Chronicle Test Site SubCatchment    Feature  ImportanceValue  \\\n",
       "0       0         0         5            1  Elevation         0.301368   \n",
       "1       0         0         5            1       Rate         0.257340   \n",
       "2       0         0         5            1         LC         0.149976   \n",
       "3       0         0         5            1        SAR         0.089709   \n",
       "4       0         0         5            1       Area         0.081249   \n",
       "..    ...       ...       ...          ...        ...              ...   \n",
       "3       0         0         5           17        SAR         0.083881   \n",
       "4       0         0         5           17       Area         0.070660   \n",
       "5       0         0         5           17      Slope         0.066038   \n",
       "6       0         0         5           17         HV         0.048680   \n",
       "7       0         0         5           17         CV         0.018755   \n",
       "\n",
       "   ImportanceRank  \n",
       "0               1  \n",
       "1               2  \n",
       "2               3  \n",
       "3               4  \n",
       "4               5  \n",
       "..            ...  \n",
       "3               4  \n",
       "4               5  \n",
       "5               6  \n",
       "6               7  \n",
       "7               8  \n",
       "\n",
       "[112 rows x 7 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features_df_all = get_features_imp_for_a_site_and_all_subcatchs(3)\n",
    "imp_features_df_all.to_csv(\"output/\" + \"test.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "for site in range(1, 41):\n",
    "    try:\n",
    "        get_features_imp_for_a_site_and_all_subcatchs(site)\n",
    "    except:\n",
    "        continue\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permut_importance(prediction_model, features_test, variable_test, site_number, subcatch_number):\n",
    "    permut = permutation_importance(prediction_model, features_test, variable_test, n_repeats=10, random_state=42)\n",
    "    print(\"permut\", permut.importances_mean)\n",
    "    sorted_idx = permut.importances_mean.argsort()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(permut.importances[sorted_idx].T,\n",
    "           vert=False, labels=features_test.columns[sorted_idx])\n",
    "    ax.set_title(\"Permutation Importances (test set)\")\n",
    "    fig.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig(\"output/PermutationImportance/\" + \"permut_imp_Site\" + str(site_number) + \"_SubCatch\" + str(subcatch_number)  +\".jpg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predition_workflow_for_all_sites_and_all_their_subcatchs():\n",
    "    for site_number in range(1,41):\n",
    "        prediction_workflow_for_a_site_for_all_its_subcatchs(site_number)    \n",
    "    print(\"Predictions are finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predition_workflow_for_all_sites_and_all_their_subcatchs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_nb in [16, 19, 20, 21, 22,25,27,28,29,33,34,35,37,38,39]:\n",
    "    prediction_workflow_for_a_site_for_all_its_subcatchs(site_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the prediction results for all the sites and all the subcatchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concat_prediction_pmax_results_with_subcatchment_for_a_site_and_all_its_subcatchs(site_number, nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "    print(\"---- Site\", site_number, \"----\")\n",
    "    frames = []\n",
    "    path = (\"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "            + \"/\"\n",
    "    )\n",
    "    for subcatch_number in range(1,31):\n",
    "        file = \"Prediction_PMax_SubCatch\" + str(subcatch_number) + \"_Chronicle\" + str(chronicle) + \"_Approx\" + str(approx) + \"_K\" + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"Clusters\" + str(nb_clusters) + \"_clean.csv\"\n",
    "        try:\n",
    "            dfp = pd.read_csv(path + file, sep=\";\")\n",
    "            print(dfp)\n",
    "            frames.append(dfp)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    #print(frames)\n",
    "    df = pd.concat(frames)\n",
    "    df.to_csv(path + \"Prediction_PMax_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "                    + str(approx)\n",
    "                    + \"_K\"\n",
    "                    + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\", index=False, sep=\";\")\n",
    "    print(\n",
    "        \"File '\"\n",
    "        + \"Prediction_PMax_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_HV_clean.csv\"\n",
    "        + \"' has been created.\")\n",
    "    \n",
    "\n",
    "def concat_prediction_h_results_with_subcatchment_for_a_site_and_all_its_subcatchs(site_number, nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "    print(\"---- Site\", site_number, \"----\")\n",
    "    frames = []\n",
    "    path = (\"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "            + \"/\"\n",
    "    )\n",
    "    for subcatch_number in range(1,31):\n",
    "        file = \"Prediction_HErrorValues_SubCatch\" + str(subcatch_number) + \"_Chronicle\" + str(chronicle) + \"_Approx\" + str(approx) + \"_K\" + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"Clusters\" + str(nb_clusters) + \"_clean.csv\"\n",
    "        try:\n",
    "            dfp = pd.read_csv(path + file, sep=\";\")\n",
    "            #print(dfp)\n",
    "            frames.append(dfp)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    #print(frames)\n",
    "    df = pd.concat(frames)\n",
    "    df.to_csv(path + \"Prediction_HErrorValues_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "                    + str(approx)\n",
    "                    + \"_K\"\n",
    "                    + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\", index=False, sep=\";\")\n",
    "    print(\n",
    "        \"File '\"\n",
    "        + \"Prediction_HErrorValues_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_HV_clean.csv\"\n",
    "        + \"' has been created.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_prediction_pmax_results_with_subcatchment_for_all_sites_by_site_and_all_its_subcatchs(nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "    for site_number in range(1,41):\n",
    "        try:\n",
    "            concat_prediction_pmax_results_with_subcatchment_for_a_site_and_all_its_subcatchs(site_number, nb_clusters, approx, chronicle, permeability)\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Concatenations finished for all sites.\")\n",
    "\n",
    "def concat_prediction_h_results_with_subcatchment_for_all_sites_by_site_and_all_its_subcatchs(nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "    for site_number in range(1,41):\n",
    "        try:\n",
    "            concat_prediction_h_results_with_subcatchment_for_a_site_and_all_its_subcatchs(site_number, nb_clusters, approx, chronicle, permeability)\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Concatenations finished for all sites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Site 1 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 2 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 3 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 4 ----\n",
      "---- Site 5 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 6 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 7 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 8 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 9 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 10 ----\n",
      "---- Site 11 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 12 ----\n",
      "---- Site 13 ----\n",
      "---- Site 14 ----\n",
      "---- Site 15 ----\n",
      "---- Site 16 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 17 ----\n",
      "---- Site 18 ----\n",
      "---- Site 19 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 20 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 21 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 22 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 23 ----\n",
      "---- Site 24 ----\n",
      "---- Site 25 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 26 ----\n",
      "---- Site 27 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 28 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 29 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 30 ----\n",
      "---- Site 31 ----\n",
      "---- Site 32 ----\n",
      "---- Site 33 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 34 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 35 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 36 ----\n",
      "---- Site 37 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 38 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 39 ----\n",
      "File 'Prediction_HErrorValues_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_HV_clean.csv' has been created.\n",
      "---- Site 40 ----\n",
      "Concatenations finished for all sites.\n"
     ]
    }
   ],
   "source": [
    "concat_prediction_h_results_with_subcatchment_for_all_sites_by_site_and_all_its_subcatchs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_prediction_pmax_results_with_subcatchment_for_all_sites_and_all_subcatchs_into_one_file(nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "        \n",
    "    frames = []\n",
    "   \n",
    "    for site_number in range(1,41):\n",
    "        path = (\"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "            + \"/\"\n",
    "            )\n",
    "        file = \"Prediction_PMax_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\" + str(approx) + \"_K\" + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\"\n",
    "        try:\n",
    "            dfp = pd.read_csv(path + file, sep=\";\")\n",
    "            frames.append(dfp)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    df = pd.concat(frames)\n",
    "    df.to_csv(\"output/\" + \"Prediction_PMax_All_Sites_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "                    + str(approx)\n",
    "                    + \"_K\"\n",
    "                    + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\", index=False, sep=\";\")\n",
    "    print(\n",
    "        \"File '\"\n",
    "        + \"Prediction_PMax_All_Sites_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV_clean.csv\"\n",
    "        + \"' has been created.\"\n",
    "    )\n",
    "\n",
    "def concat_prediction_h_results_with_subcatchment_for_all_sites_and_all_subcatchs_into_one_file(nb_clusters=1, approx=0, chronicle=0, permeability=27.32):\n",
    "        \n",
    "    frames = []\n",
    "   \n",
    "    for site_number in range(1,41):\n",
    "        path = (\"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "            + \"/\"\n",
    "            )\n",
    "        file = \"Prediction_HErrorValues_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\" + str(approx) + \"_K\" + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\"\n",
    "        try:\n",
    "            dfp = pd.read_csv(path + file, sep=\";\")\n",
    "            frames.append(dfp)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    df = pd.concat(frames)\n",
    "    df.to_csv(\"output/\" + \"Prediction_HErrorValues_All_Sites_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "                    + str(approx)\n",
    "                    + \"_K\"\n",
    "                    + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_HV\" + \"_Clusters\" + str(nb_clusters) + \"_clean.csv\", index=False, sep=\";\")\n",
    "    print(\n",
    "        \"File '\"\n",
    "        + \"Prediction_HErrorValues_All_Sites_All_SubCatchs_Chronicle\" + str(chronicle) + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability) + \"_Slope_Elevation_LC_SAR_Area_CV_clean.csv\"\n",
    "        + \"' has been created.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Prediction_PMax_All_Sites_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_clean.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "concat_prediction_pmax_results_with_subcatchment_for_all_sites_and_all_subcatchs_into_one_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Prediction_HErrorValues_All_Sites_All_SubCatchs_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_clean.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "concat_prediction_h_results_with_subcatchment_for_all_sites_and_all_subcatchs_into_one_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_fig_perm_import(approx, chronicle, result, test_site, X_test, nse_test):\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "    ax.set_title(\"Permutation Importances (test set)\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"output/PermutationImportance/\" + \"/ZLearning/Approx\" + str(approx) + \"/Chronicle\" + str(chronicle) + \"/PermutationImportance_NbFeatures\" + str(len(X_test.columns)) + \"_TestSite\" + str(test_site) + \"_NSE\" + str(nse_test) + \".jpg\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def permutation_importance_for_a_site(approx, chronicle, test_site, X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = get_train_and_test_data(X, y, test_site)\n",
    "    \n",
    "    # Build a forest and compute the feature importances\n",
    "    forest = RandomForestRegressor(n_estimators=1000, criterion=\"mse\", random_state=1, n_jobs=-1)\n",
    "    forest.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    print(\"RF train accuracy: %0.3f\" % forest.score(X_train, y_train))\n",
    "    print(\"RF test accuracy: %0.3f\" % forest.score(X_test, y_test))\n",
    "    nse_test = round(forest.score(X_test, y_test), 3)\n",
    "\n",
    "    \n",
    "\n",
    "    result = permutation_importance(forest, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "    \n",
    "    get_fig_perm_import(approx, chronicle, result, test_site, X_test, nse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance : https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names, 'feature_importance': importances}).sort_values('feature_importance', ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "def drop_col_feat_imp(model, X_train, y_train, random_state = 1):\n",
    "    \n",
    "    # clone the model to have the exact same specification as the one initially trained\n",
    "    model_clone = clone(model)\n",
    "    # set random_state for comparability\n",
    "    model_clone.random_state = random_state\n",
    "    # training and scoring the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    # list for storing feature importances\n",
    "    importances = []\n",
    "    \n",
    "    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "    \n",
    "    importances_df = imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = drop_col_feat_imp(prediction_model, features_train, variable_train.values.ravel(), random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rate</td>\n",
       "      <td>0.534919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slope</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Area</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CV</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SAR</td>\n",
       "      <td>-0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elevation</td>\n",
       "      <td>-0.000186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LC</td>\n",
       "      <td>-0.000489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature  feature_importance\n",
       "0       Rate            0.534919\n",
       "1      Slope            0.000069\n",
       "2         HV            0.000060\n",
       "3       Area            0.000049\n",
       "4         CV            0.000016\n",
       "5        SAR           -0.000033\n",
       "6  Elevation           -0.000186\n",
       "7         LC           -0.000489"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features_df_all = pd.DataFrame(columns=[\"feature\", \"feature_importance\", \"site_test\"])\n",
    "\n",
    "for site_number in [1, 2, 3, 5,6,7,8,9,11,16,19,20,21,22,25,27,28,29,33,34,35,37]: # [1, 2, 3, 5,6,7,8,9,11,16,19,20,21,22,25,27,28,29,33,34,35,37]\n",
    "    print(\"site: \", site_number)\n",
    "    features, variable_to_predict = prediction.split_dataset_into_features_and_variable_to_predict(input_data_cleaned)\n",
    "    features_train, features_test, variable_train, variable_test = prediction.get_train_and_test_data(features, variable_to_predict, site_number)\n",
    "    prediction_model = train_random_forest_model(features_train, variable_train)\n",
    "    variable_train_pred, variable_test_pred = predict_with_trained_model(prediction_model, features_train, features_test)\n",
    "    subCatchment_numbers = prediction.get_subcatchment_numbers_for_a_site(site_number, variable_to_predict)\n",
    "    liste_variable_test_HError = prediction.get_list_variable_test_Hind_Values(variable_test)\n",
    "    liste_variable_test_pred_HError = variable_test_pred\n",
    "    mse_test, r2_test = prediction.get_standard_quality_metrics(subCatchment_numbers, liste_variable_test_HError, liste_variable_test_pred_HError)\n",
    "    importances_df = drop_col_feat_imp(prediction_model, features_train, variable_train.values.ravel(), random_state = 1)\n",
    "    importances_df[\"site_test\"] = site_number\n",
    "    imp_features_df_all = pd.concat([imp_features_df_all, importances_df])\n",
    "imp_features_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features_df_all.to_csv(\"output/\" + \"Feature_Importance_RF_SitesTests_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
