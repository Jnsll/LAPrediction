{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import prediction_of_H_indicator_with_subCatchmentData as prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>SubCatch</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>LC</th>\n",
       "      <th>SAR</th>\n",
       "      <th>Area</th>\n",
       "      <th>CV</th>\n",
       "      <th>HV</th>\n",
       "      <th>HError</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.035656</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.083538</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.203898</td>\n",
       "      <td>8.936689</td>\n",
       "      <td>319.676414</td>\n",
       "      <td>1.000254</td>\n",
       "      <td>1648125</td>\n",
       "      <td>167</td>\n",
       "      <td>211</td>\n",
       "      <td>0.116415</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>8.758309</td>\n",
       "      <td>162.546722</td>\n",
       "      <td>690.708947</td>\n",
       "      <td>1.006050</td>\n",
       "      <td>2925000</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>7.371158</td>\n",
       "      <td>150.872406</td>\n",
       "      <td>470.062314</td>\n",
       "      <td>1.004910</td>\n",
       "      <td>8943750</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>4.720726</td>\n",
       "      <td>195.869247</td>\n",
       "      <td>672.920859</td>\n",
       "      <td>1.001445</td>\n",
       "      <td>1299375</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "      <td>6.162337</td>\n",
       "      <td>121.253716</td>\n",
       "      <td>589.868107</td>\n",
       "      <td>1.003319</td>\n",
       "      <td>9225000</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>5.605823</td>\n",
       "      <td>217.943283</td>\n",
       "      <td>640.819977</td>\n",
       "      <td>1.001974</td>\n",
       "      <td>5394375</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3435 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Site  SubCatch     Slope   Elevation          LC       SAR     Area  \\\n",
       "0        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "4        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "8        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "9        1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "11       1         1  1.203898    8.936689  319.676414  1.000254  1648125   \n",
       "...    ...       ...       ...         ...         ...       ...      ...   \n",
       "6603    39         4  8.758309  162.546722  690.708947  1.006050  2925000   \n",
       "6633    39         5  7.371158  150.872406  470.062314  1.004910  8943750   \n",
       "6664    39         7  4.720726  195.869247  672.920859  1.001445  1299375   \n",
       "6694    39         8  6.162337  121.253716  589.868107  1.003319  9225000   \n",
       "6724    39         9  5.605823  217.943283  640.819977  1.001974  5394375   \n",
       "\n",
       "       CV   HV    HError   Rate  \n",
       "0     167  211  0.000000    1.0  \n",
       "4     167  211  0.035656   21.0  \n",
       "8     167  211  0.083538   60.0  \n",
       "9     167  211  0.093477   75.0  \n",
       "11    167  211  0.116415  100.0  \n",
       "...   ...  ...       ...    ...  \n",
       "6603    0   27  0.000000    1.0  \n",
       "6633    0  127  0.000000    1.0  \n",
       "6664    0    5  0.000000    1.0  \n",
       "6694    0   61  0.000000    1.0  \n",
       "6724    0   26  0.000000    1.0  \n",
       "\n",
       "[3435 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = prediction.import_input_data_clean()\n",
    "input_data_cleaned = prediction.clean_data(input_data)\n",
    "input_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_by_rate = {}\n",
    "for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 125, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "\n",
    "    input_by_rate[rate] = input_data_cleaned[input_data_cleaned[\"Rate\"]== float(rate)]\n",
    "#input_by_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import prediction_of_H_indicator_with_subCatchmentData as prediction\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#input_data_for_rate = input_by_rate[rate]\n",
    "def pred_by_rate_for_a_site(site_number, subcatch_number, rate, input_data_for_rate):\n",
    "    features, variable_to_predict = prediction.split_dataset_into_features_and_variable_to_predict(input_data_for_rate)\n",
    "    features_train, features_test, variable_train, variable_test = prediction.get_train_and_test_data_for_a_subcatch(features, variable_to_predict, site_number, subcatch_number)\n",
    "#print(features_train, features_test, variable_train, variable_test)\n",
    "    prediction_model = prediction.train_random_forest_model(features_train, variable_train)\n",
    "    variable_train_pred, variable_test_pred = prediction.predict_with_trained_model(prediction_model, features_train, features_test)\n",
    "#print(variable_train_pred, variable_test_pred)\n",
    "    subCatchment_numbers = prediction.get_subcatchment_numbers_for_a_subcatch(site_number, variable_to_predict, subcatch_number)\n",
    "    liste_variable_test_HError = prediction.get_list_variable_test_Hind_Values(variable_test)\n",
    "    liste_variable_test_pred_HError = variable_test_pred\n",
    "    #mse_test, r2_test = prediction.get_standard_quality_metrics(subCatchment_numbers, liste_variable_test_HError, liste_variable_test_pred_HError)\n",
    "    #print(mse_test, r2_test)\n",
    "    #rates = prediction.get_rates_for_a_subcatch(site_number, features, subcatch_number)\n",
    "    #pmax_test, pmax_pred = prediction.get_real_and_pred_pmax(subCatchment_numbers, rates, liste_variable_test_HError, variable_test_pred)\n",
    "    #print(pmax_test, pmax_pred)\n",
    "    print(liste_variable_test_HError)\n",
    "    print(variable_test_pred)\n",
    "    prediction.save_Hind_results_into_file_for_a_subcatch_byRates(site_number, subcatch_number, rate, 1,liste_variable_test_HError, variable_test_pred, approx=0, chronicle=0, permeability=27.32)\n",
    "    #prediction.save_Pmax_results_into_file_for_a_subcatch_byRates(site_number, subcatch_number, 1, pmax_test, pmax_pred, mse_test, r2_test, approx=0, chronicle=0, permeability=27.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "[0.122143552048743]\n",
      "[0.17006608]\n",
      "debut function!\n",
      "output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch9_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HVClusters1_clean_byRates.csv\n",
      "File created here:  output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch9_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HV_clean_byRates.csv\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "[0.028080137863207]\n",
      "[0.11362716]\n",
      "debut function!\n",
      "output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch10_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HVClusters1_clean_byRates.csv\n",
      "File created here:  output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch10_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HV_clean_byRates.csv\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "[0.079912444783554]\n",
      "[0.0832615]\n",
      "debut function!\n",
      "output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch11_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HVClusters1_clean_byRates.csv\n",
      "File created here:  output/Approx0/Chronicle0/SiteTest9/Prediction_HErrorValues_SubCatch11_Rate182_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HV_clean_byRates.csv\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n",
      "Site 9\n"
     ]
    }
   ],
   "source": [
    "for site_number in range(9,10):\n",
    "    for subcatch_number in range(1,31):\n",
    "        for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "            print(\"Site\", site_number)\n",
    "            try:\n",
    "                prediction_model, features_train, variable_train = pred_by_rate_for_a_site(site_number, subcatch_number, rate, input_by_rate[rate])\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def concat_H_by_subcatch_by_rates(site_number,subcatch_number,approx=0, chronicle=0, permeability=27.32, nb_clusters=1):\n",
    "    \n",
    "    MYDIR = (\n",
    "    \"output\"\n",
    "    + \"/\"\n",
    "    + \"Approx\"\n",
    "    + str(approx)\n",
    "    + \"/Chronicle\"\n",
    "    + str(chronicle)\n",
    "    + \"/SiteTest\"\n",
    "    + str(site_number)\n",
    "    )\n",
    "\n",
    "    df_rates = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'Rate', 'H Error Real', 'H Error Predict'])\n",
    "    for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 125, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "\n",
    "        try:\n",
    "            df_rate = pd.read_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch\"\n",
    "        + str(subcatch_number)\n",
    "        + \"_Rate\"\n",
    "        + str(rate)\n",
    "        + \"_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_clean_byRates.csv\", sep=\";\")\n",
    "\n",
    "            df_rates = pd.concat([df_rates, df_rate])\n",
    "        except:\n",
    "            continue\n",
    "    df_rates.to_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch\"+ str(subcatch_number) + \"_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\" , index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_H_by_subcatch_by_rates(2,3,approx=0, chronicle=0, permeability=27.32, nb_clusters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_h_val_for_a_site_by_rates(site_number,approx=0, chronicle=0, permeability=27.32, nb_clusters=1):\n",
    "    MYDIR = (\n",
    "    \"output\"\n",
    "    + \"/\"\n",
    "    + \"Approx\"\n",
    "    + str(approx)\n",
    "    + \"/Chronicle\"\n",
    "    + str(chronicle)\n",
    "    + \"/SiteTest\"\n",
    "    + str(site_number)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df_rates = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'Rate', 'H Error Real', 'H Error Predict'])\n",
    "    for subcatch_number in range(1, 21):\n",
    "        try:\n",
    "            df_rate = pd.read_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch\"+ str(subcatch_number) + \"_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\")\n",
    "            df_rates = pd.concat([df_rates, df_rate])\n",
    "        except:\n",
    "            continue\n",
    "    df_rates.to_csv(MYDIR + \"/\" + \"Prediction_HErrorValues_All_SubCatch\"+ \"_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\" , sep = \";\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_h_val_for_a_site_by_rates(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_plot_H_value_by_rates(site_number, subcatch_number, nb_clusters=1, chronicle=0, approx=0, permeability=27.32):\n",
    "\n",
    "    MYDIR = (\n",
    "            \"output\"\n",
    "            + \"/\"\n",
    "            + \"Approx\"\n",
    "            + str(approx)\n",
    "            + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"/SiteTest\"\n",
    "            + str(site_number)\n",
    "        )\n",
    "    \n",
    "    H_df = pd.read_csv(\"output/\" + \"Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\")\n",
    "    df_site = H_df.loc[H_df[\"Test Site\"]== site_number]\n",
    "    df_sub = df_site.loc[df_site[\"SubCatchment\"]==subcatch_number]\n",
    "    #print(df_sub)\n",
    "    df_sub.plot(x=\"Rate\", y=\"H Error Predict\")\n",
    "    plt.savefig(MYDIR + \"/\" + \"Plot_Site\" + str(site_number) + \"_SubCatch\" + str(subcatch_number) + \"H_Pred_byRates.jpg\")\n",
    "    #plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_number in range(1,41):\n",
    "    for subcatch_number in range(1,41):\n",
    "        print(\"Site\", site_number, \"Subcatch\", subcatch_number)\n",
    "        try:\n",
    "            get_plot_H_value_by_rates(site_number,subcatch_number)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_number in range(1,41):\n",
    "    print(\"----\", site_number, \"----\")\n",
    "    for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 125, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "        try:\n",
    "            pred_by_rate_for_a_site(site_number, rate, input_by_rate[rate])\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_max_for_site_byRates(site_number, approx=0, chronicle=0, permeability=27.32, nb_clusters=1):\n",
    "    MYDIR = (\n",
    "    \"output\"\n",
    "    + \"/\"\n",
    "    + \"Approx\"\n",
    "    + str(approx)\n",
    "    + \"/Chronicle\"\n",
    "    + str(chronicle)\n",
    "    + \"/SiteTest\"\n",
    "    + str(site_number)\n",
    "    )\n",
    "\n",
    "    df_rates = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'Rate', 'H Error Real', 'H Error Predict'])\n",
    "    for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 125, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "\n",
    "        try:\n",
    "            df_rate = pd.read_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch_Chronicle\" + str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"\n",
    "            + str(permeability)\n",
    "            + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "            + \"Clusters\" + str(nb_clusters) + \"_Rate\" + str(rate) + \".csv\", sep=\";\")\n",
    "\n",
    "            df_rates = pd.concat([df_rates, df_rate])\n",
    "        except:\n",
    "            continue\n",
    "    df_rates.to_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\" , index=False)\n",
    "    \n",
    "    subCatchment_numbers = df_rates[\"SubCatchment\"].max()\n",
    "    \n",
    "    \n",
    "    f = open(MYDIR\n",
    "        + \"/\"\n",
    "        + \"Prediction_PMax_SubCatch_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", 'w')\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "    writer.writerow(\n",
    "            [\n",
    "                \"Approx\",\n",
    "                \"Chronicle\",\n",
    "                \"Test Site\",\n",
    "                \"SubCatchment\",\n",
    "                \"P Real\",\n",
    "                \"P pred\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    for sub in range(1, subCatchment_numbers+1):\n",
    "        df_sub = df_rates[df_rates[\"SubCatchment\"]== sub]\n",
    "        p_real_found = False\n",
    "        p_pred_found = False\n",
    "        p_real = 3652\n",
    "        p_pred = 3652\n",
    "        for ind in range(len(df_sub)):\n",
    "            if df_sub.iloc[ind, 5] > 0.1 and p_real_found == False:\n",
    "                p_real = df_sub.iloc[ind, 4]\n",
    "                p_real_found = True\n",
    "                break\n",
    "        for ind in range(len(df_sub)):\n",
    "            if df_sub.iloc[ind, 6] > 0.1 and p_pred_found == False:\n",
    "                p_pred = df_sub.iloc[ind, 4]\n",
    "                p_pred_found = True\n",
    "                break\n",
    "        writer.writerow(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    site_number,\n",
    "                    sub,\n",
    "                    p_real,\n",
    "                    p_pred,\n",
    "                ]\n",
    "            )\n",
    "        #print(p_real, p_pred)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in range(1, 41):\n",
    "    try:\n",
    "        get_p_max_for_site_byRates(site)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concat_all_H_by_rates(approx=0, chronicle=0, permeability=27.32, nb_clusters=1):\n",
    "    df_all = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'Rate', 'H Error Real', 'H Error Predict'])\n",
    "    for site_number in range(1,41):\n",
    "        MYDIR = (\n",
    "        \"output\"\n",
    "        + \"/\"\n",
    "        + \"Approx\"\n",
    "        + str(approx)\n",
    "        + \"/Chronicle\"\n",
    "            + str(chronicle)\n",
    "        + \"/SiteTest\"\n",
    "        + str(site_number)\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            df_site = pd.read_csv(MYDIR\n",
    "            + \"/\"\n",
    "            + \"Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\",\")\n",
    "            df_all = pd.concat([df_all, df_site])\n",
    "        except:\n",
    "            print(\"site without file\", site_number)\n",
    "            continue\n",
    "    #print(df_all)\n",
    "    df_all.to_csv(\"output/\" + \"Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\", index=False)\n",
    "\n",
    "concat_all_H_by_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site without data 4\n",
      "site without data 10\n",
      "site without data 12\n",
      "site without data 13\n",
      "site without data 14\n",
      "site without data 15\n",
      "site without data 17\n",
      "site without data 18\n",
      "site without data 23\n",
      "site without data 24\n",
      "site without data 26\n",
      "site without data 30\n",
      "site without data 31\n",
      "site without data 32\n",
      "site without data 36\n",
      "site without data 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def concat_all_P_by_rates(approx=0, chronicle=0, permeability=27.32, nb_clusters=1):\n",
    "\n",
    "\n",
    "    df_all_p = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'P Real', 'P pred'])\n",
    "\n",
    "    for site_number in range(1,41):\n",
    "        MYDIR = (\n",
    "        \"output\"\n",
    "        + \"/\"\n",
    "        + \"Approx\"\n",
    "        + str(approx)\n",
    "        + \"/Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"/SiteTest\"\n",
    "        + str(site_number)\n",
    "        )\n",
    "        try:\n",
    "            df_site = pd.read_csv(MYDIR\n",
    "            + \"/\"\n",
    "            + \"Prediction_PMax_SubCatch_Chronicle\"\n",
    "            + str(chronicle)\n",
    "            + \"_Approx\"\n",
    "            + str(approx)\n",
    "            + \"_K\"\n",
    "            + str(permeability)\n",
    "            + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "            + \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\")\n",
    "\n",
    "            df_all_p = pd.concat([df_all_p, df_site])\n",
    "        except:\n",
    "            print(\"site without data\", site_number)\n",
    "            continue\n",
    "    df_all_p.to_csv(\"output/\" + \"Prediction_PMax_SubCatch_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\", index=False)\n",
    "\n",
    "concat_all_P_by_rates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11969981810290574\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "chronicle= 0\n",
    "approx = 0\n",
    "permeability = 27.32\n",
    "nb_clusters=1\n",
    "\n",
    "\n",
    "df_p = pd.read_csv(\"output/\" + \"Prediction_PMax_SubCatch_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\")\n",
    "\n",
    "df_H = pd.read_csv(\"output/Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", sep=\";\")\n",
    "\n",
    "#print(df_H, df_p)\n",
    "\n",
    "metric_H_values = []\n",
    "for row in range(len(df_p)):\n",
    "    #print(row)\n",
    "    p_pred = df_p.iloc[row, 5]\n",
    "    sub = df_p.iloc[row, 3]\n",
    "    site = df_p.iloc[row, 2]\n",
    "    \n",
    "    df_value = df_H.loc[(df_H[\"Test Site\"] == site) & (df_H[\"SubCatchment\"] == sub) & (df_H[\"Rate\"] == p_pred)] \n",
    "    if df_value.empty is False:\n",
    "        metric_H_values.append(df_value.iloc[0,5])\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"no H data for site:\", site, \"Sub:\", sub)\n",
    "\n",
    "print(statistics.mean(metric_H_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "approx=0\n",
    "chronicle=0\n",
    "permeability=27.32\n",
    "site_number = 1\n",
    "#rate=2\n",
    "nb_clusters=1\n",
    "\n",
    "MYDIR = (\n",
    "    \"output\"\n",
    "    + \"/\"\n",
    "    + \"Approx\"\n",
    "    + str(approx)\n",
    "    + \"/Chronicle\"\n",
    "    + str(chronicle)\n",
    "    + \"/SiteTest\"\n",
    "    + str(site_number)\n",
    "    )\n",
    "\n",
    "df_rates = pd.DataFrame(columns=['Approx', 'Chronicle', 'Test Site', 'SubCatchment', 'Rate', 'H Error Real', 'H Error Predict'])\n",
    "for rate in [2, 7, 15, 21, 30, 45, 50, 60, 75, 90, 100, 125, 150, 182, 200, 250, 300, 330, 350, 365, 550, 640, 730, 1000, 1500, 2000, 2250, 3000, 3182, 3652]:\n",
    "\n",
    "    try:\n",
    "        df_rate = pd.read_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch_Chronicle\" + str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_Rate\" + str(rate) + \".csv\", sep=\";\")\n",
    "\n",
    "        df_rates = pd.concat([df_rates, df_rate])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rates.to_csv(MYDIR + \"/\"+ \"Prediction_HErrorValues_SubCatch_Chronicle\"+ str(chronicle)+ \"_Approx\"+ str(approx)+ \"_K\"+ str(permeability)+ \"_Slope_Elevation_LC_SAR_Area_CV_HV\"+ \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "subCatchment_numbers = df_rates[\"SubCatchment\"].max()\n",
    "print(subCatchment_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3652\n",
      "3652 3652\n",
      "125 3652\n",
      "3652 60\n",
      "3652 75\n",
      "125 3652\n",
      "60 3652\n",
      "3652 75\n",
      "3652 90\n",
      "3652 182\n",
      "125 3652\n",
      "365 3652\n",
      "100 3652\n",
      "150 3652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "f = open(MYDIR\n",
    "        + \"/\"\n",
    "        + \"Prediction_PMax_SubCatch_Chronicle\"\n",
    "        + str(chronicle)\n",
    "        + \"_Approx\"\n",
    "        + str(approx)\n",
    "        + \"_K\"\n",
    "        + str(permeability)\n",
    "        + \"_Slope_Elevation_LC_SAR_Area_CV_HV\"\n",
    "        + \"Clusters\" + str(nb_clusters) + \"_byRates\" + \".csv\", 'w')\n",
    "writer = csv.writer(f, delimiter=\";\")\n",
    "writer.writerow(\n",
    "            [\n",
    "                \"Approx\",\n",
    "                \"Chronicle\",\n",
    "                \"Test Site\",\n",
    "                \"SubCatchment\",\n",
    "                \"P Real\",\n",
    "                \"P pred\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "for sub in range(1, subCatchment_numbers+1):\n",
    "    df_sub = df_rates[df_rates[\"SubCatchment\"]== sub]\n",
    "    p_real_found = False\n",
    "    p_pred_found = False\n",
    "    p_real = 3652\n",
    "    p_pred = 3652\n",
    "    for ind in range(len(df_sub)):\n",
    "        if df_sub.iloc[ind, 5] > 0.1 and p_real_found == False:\n",
    "            p_real = df_sub.iloc[ind, 4]\n",
    "            p_real_found = True\n",
    "            break\n",
    "        if df_sub.iloc[ind, 6] > 0.1 and p_pred_found == False:\n",
    "            p_pred = df_sub.iloc[ind, 4]\n",
    "            p_pred_found = True\n",
    "            break\n",
    "    writer.writerow(\n",
    "                [\n",
    "                    0,\n",
    "                    0,\n",
    "                    site_number,\n",
    "                    sub,\n",
    "                    p_real,\n",
    "                    p_pred,\n",
    "                ]\n",
    "            )\n",
    "    print(p_real, p_pred)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created here:  output/Approx0/Chronicle0/SiteTest1/Prediction_HErrorValues_SubCatch_Chronicle0_Approx0_K27.32_Slope_Elevation_LC_SAR_Area_CV_HVClusters1_Rate2.csv\n"
     ]
    }
   ],
   "source": [
    "save_Hind_results_into_file_by_rate(site_number, 1, subCatchment_numbers, rate, liste_variable_test_HError, variable_test_pred, approx=0, chronicle=0, permeability=27.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "def drop_col_feat_imp(model, X_train, y_train, random_state = 1):\n",
    "    \n",
    "    # clone the model to have the exact same specification as the one initially trained\n",
    "    model_clone = clone(model)\n",
    "    # set random_state for comparability\n",
    "    model_clone.random_state = random_state\n",
    "    # training and scoring the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    # list for storing feature importances\n",
    "    importances = []\n",
    "    \n",
    "    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "    \n",
    "    importances_df = imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
